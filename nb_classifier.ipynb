{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sophiehuang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sophiehuang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xff in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 178\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m預測結果已保存到 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# 執行主程式\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 151\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./predictions3.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# 讀取文檔和訓練資料\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mread_documents_from_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m class_docs \u001b[38;5;241m=\u001b[39m read_training_file(training_file_path)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# 測試文檔是未在訓練集中出現的文檔\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m, in \u001b[0;36mread_documents_from_folder\u001b[0;34m(folder_path)\u001b[0m\n\u001b[1;32m     35\u001b[0m         doc_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(filename\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, filename), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m---> 37\u001b[0m             documents[doc_id] \u001b[38;5;241m=\u001b[39m \u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m documents\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m data[consumed:]\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "## vincent naive bayes\n",
    "\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import math\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 移除第三行\n",
    "    lines = text.split('\\n')\n",
    "    if len(lines) > 2:\n",
    "        text = '\\n'.join(lines[:2] + lines[3:])\n",
    "    # 轉換為小寫\n",
    "    text = text.lower()\n",
    "    # 移除標點符號\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # 移除指定單詞和停用詞\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words and word != 'embed']\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# 讀取文檔資料夾中的文檔\n",
    "def read_documents_from_folder(folder_path):\n",
    "    documents = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            doc_id = int(filename.split('.')[0])\n",
    "            with open(os.path.join(folder_path, filename), \"r\", encoding=\"utf-8\", errors = \"replace\") as file:\n",
    "                documents[doc_id] = file.read()\n",
    "    return documents\n",
    "\n",
    "# 讀取訓練檔案\n",
    "def read_training_file(training_file_path):\n",
    "    class_docs = {}\n",
    "    with open(training_file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            class_id = int(parts[0])\n",
    "            doc_ids = list(map(int, parts[1:]))\n",
    "            if class_id not in class_docs:\n",
    "                class_docs[class_id] = []\n",
    "            class_docs[class_id].extend(doc_ids)\n",
    "    return class_docs\n",
    "\n",
    "# 增加字典字數\n",
    "def safe_increment(dictionary, key):\n",
    "    if key not in dictionary:\n",
    "        dictionary[key] = 0\n",
    "    dictionary[key] += 1\n",
    "\n",
    "# 計算詞頻\n",
    "def calculate_term_frequencies(documents, class_docs):\n",
    "    class_term_freq = {}\n",
    "    overall_term_freq = {}\n",
    "\n",
    "    for class_id, doc_ids in class_docs.items():\n",
    "        if class_id not in class_term_freq:\n",
    "            class_term_freq[class_id] = {}\n",
    "\n",
    "        for doc_id in doc_ids:\n",
    "            if doc_id in documents:\n",
    "                tokens = preprocess_text(documents[doc_id])\n",
    "                for token in tokens:\n",
    "                    safe_increment(class_term_freq[class_id], token)\n",
    "                    safe_increment(overall_term_freq, token)\n",
    "\n",
    "    return class_term_freq, overall_term_freq\n",
    "\n",
    "# 卡方檢定計算特徵選擇\n",
    "def chi2_feature_selection(class_term_freq, overall_term_freq, class_docs, total_classes, top_k=500):\n",
    "    chi2_scores = {}\n",
    "    total_docs = sum(len(docs) for docs in class_docs.values())\n",
    "\n",
    "    for term, total_count in overall_term_freq.items():\n",
    "        for class_id in range(1, total_classes + 1):\n",
    "            A = class_term_freq.get(class_id, {}).get(term, 0)\n",
    "            B = sum(class_term_freq.get(c, {}).get(term, 0) for c in range(1, total_classes + 1)) - A\n",
    "            C = len(class_docs.get(class_id, [])) - A\n",
    "            D = total_docs - (A + B + C)\n",
    "\n",
    "            numerator = total_docs * (A * D - B * C) ** 2\n",
    "            denominator = (A + C) * (B + D) * (A + B) * (C + D)\n",
    "            if denominator > 0:\n",
    "                if term not in chi2_scores:\n",
    "                    chi2_scores[term] = 0\n",
    "                chi2_scores[term] += numerator / denominator\n",
    "\n",
    "    sorted_terms = sorted(chi2_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    selected_terms = {term: idx for idx, (term, _) in enumerate(sorted_terms[:top_k])}\n",
    "    return selected_terms\n",
    "\n",
    "# 計算機率\n",
    "def calculate_class_probabilities(documents, class_docs, vocabulary):\n",
    "    class_word_counts = {}\n",
    "    class_doc_counts = {}\n",
    "    total_docs = sum(len(docs) for docs in class_docs.values())\n",
    "\n",
    "    for class_id, doc_ids in class_docs.items():\n",
    "        if class_id not in class_word_counts:\n",
    "            class_word_counts[class_id] = {}\n",
    "        class_doc_counts[class_id] = len(doc_ids)\n",
    "\n",
    "        for doc_id in doc_ids:\n",
    "            if doc_id in documents:\n",
    "                tokens = preprocess_text(documents[doc_id])\n",
    "                for token in tokens:\n",
    "                    if token in vocabulary:\n",
    "                        safe_increment(class_word_counts[class_id], token)\n",
    "\n",
    "    class_priors = {class_id: count / total_docs for class_id, count in class_doc_counts.items()}\n",
    "    class_conditional_probs = {}\n",
    "\n",
    "    for class_id, word_counts in class_word_counts.items():\n",
    "        total_words = sum(word_counts.values()) + len(vocabulary)\n",
    "        class_conditional_probs[class_id] = {\n",
    "            term: (word_counts.get(term, 0) + 1) / total_words for term in vocabulary\n",
    "        }\n",
    "    return class_priors, class_conditional_probs\n",
    "\n",
    "# 分類測試文檔\n",
    "def classify_documents(test_documents, vocabulary, class_priors, class_conditional_probs):\n",
    "    results = {}\n",
    "    for doc_id, doc_content in test_documents.items():\n",
    "        tokens = preprocess_text(doc_content)\n",
    "        tokens = [token for token in tokens if token in vocabulary]\n",
    "        class_scores = {}\n",
    "        for class_id, prior in class_priors.items():\n",
    "            log_prob = math.log(prior)\n",
    "            for token in tokens:\n",
    "                log_prob += math.log(class_conditional_probs[class_id].get(token, 1e-6))\n",
    "            class_scores[class_id] = log_prob\n",
    "        predicted_class = max(class_scores, key=class_scores.get)\n",
    "        results[doc_id] = predicted_class\n",
    "    return results\n",
    "\n",
    "# 主程式\n",
    "def main():\n",
    "    folder_path = '/Users/sophiehuang/Documents/113-1/113-1-IRTM/all_lyrics'\n",
    "    training_file_path = '/Users/sophiehuang/Documents/113-1/113-1-IRTM/113-1-IRTM-final-project/folds/fold3.txt'\n",
    "    output_file = './predictions3.csv'\n",
    "\n",
    "    # 讀取文檔和訓練資料\n",
    "    documents = read_documents_from_folder(folder_path)\n",
    "    class_docs = read_training_file(training_file_path)\n",
    "\n",
    "    # 測試文檔是未在訓練集中出現的文檔\n",
    "    training_ids = set(doc_id for doc_ids in class_docs.values() for doc_id in doc_ids)\n",
    "    test_documents = {doc_id: doc for doc_id, doc in documents.items() if doc_id not in training_ids}\n",
    "\n",
    "    # 計算詞頻與選擇特徵\n",
    "    class_term_freq, overall_term_freq = calculate_term_frequencies(documents, class_docs)\n",
    "    vocabulary = chi2_feature_selection(class_term_freq, overall_term_freq, class_docs, len(class_docs), top_k=500)\n",
    "\n",
    "    # 計算先驗機率與條件機率\n",
    "    class_priors, class_conditional_probs = calculate_class_probabilities(documents, class_docs, vocabulary)\n",
    "\n",
    "    # 分類測試文檔\n",
    "    predictions = classify_documents(test_documents, vocabulary, class_priors, class_conditional_probs)\n",
    "\n",
    "    # 保存結果\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Id\", \"Value\"])\n",
    "        for doc_id, predicted_class in sorted(predictions.items()):\n",
    "            writer.writerow([doc_id, predicted_class])\n",
    "\n",
    "    print(f\"預測結果已保存到 {output_file}\")\n",
    "\n",
    "# 執行主程式\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      file  precision    recall  f1_score\n",
      "0   output_svm_param_3.csv   0.247520  0.274454  0.243258\n",
      "1   output_svm_param_2.csv   0.255037  0.273712  0.241058\n",
      "2   output_svm_param_1.csv   0.248988  0.280773  0.253086\n",
      "3   output_svm_param_5.csv   0.256993  0.276730  0.241925\n",
      "4   output_svm_param_4.csv   0.262353  0.276816  0.252420\n",
      "5   output_svm_param_6.csv   0.245009  0.274933  0.243543\n",
      "6   output_svm_param_7.csv   0.236972  0.276687  0.243191\n",
      "7  output_svm_param_10.csv   0.250025  0.282447  0.253279\n",
      "8   output_svm_param_9.csv   0.263737  0.275665  0.247597\n",
      "9   output_svm_param_8.csv   0.251372  0.273212  0.245497\n"
     ]
    }
   ],
   "source": [
    "## calculate scores\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# 定義正確答案檔案和預測結果資料夾\n",
    "# 定義正確答案檔案和預測結果資料夾\n",
    "ground_truth_path = '/Users/sophiehuang/Documents/113-1/113-1-IRTM/113-1-IRTM-final-project/output_modified.csv'\n",
    "predictions_folder = '/Users/sophiehuang/Documents/113-1/113-1-IRTM/113-1-IRTM-final-project/prediction_sophie'\n",
    "\n",
    "# 讀取正確答案\n",
    "ground_truth = pd.read_csv(ground_truth_path)\n",
    "\n",
    "# 結果存放列表\n",
    "results = []\n",
    "\n",
    "# 遍歷預測檔案\n",
    "for prediction_file in os.listdir(predictions_folder):\n",
    "    if prediction_file.endswith('.csv'):  # 僅處理 CSV 檔案\n",
    "        # 讀取預測檔案\n",
    "        predictions_path = os.path.join(predictions_folder, prediction_file)\n",
    "        predictions = pd.read_csv(predictions_path)\n",
    "\n",
    "        # 合併正確答案和預測檔案，僅保留相同的 Id\n",
    "        merged = pd.merge(ground_truth, predictions, on='Id', how='inner')\n",
    "\n",
    "        if merged.empty:  # 如果沒有共同的 Id，跳過該檔案\n",
    "            print(f\"Skipping file {prediction_file}: No matching Ids.\")\n",
    "            continue\n",
    "\n",
    "        # 提取正確和預測的值\n",
    "        true_labels = merged['Value_x']  # 正確答案中的 'Value'\n",
    "        predicted_labels = merged['Value_y']  # 預測檔案中的 'Value'\n",
    "\n",
    "        # 計算 Precision, Recall, 和 F1-score\n",
    "        precision = precision_score(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "        recall = recall_score(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "\n",
    "        # 將結果加入列表\n",
    "        results.append({\n",
    "            'file': prediction_file,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1\n",
    "        })\n",
    "\n",
    "# 將結果轉為 DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# 顯示結果並保存到檔案\n",
    "print(results_df)\n",
    "results_df.to_csv('evaluation_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "處理完成，結果已儲存到 'output.csv'\n"
     ]
    }
   ],
   "source": [
    "## change output\n",
    "import pandas as pd\n",
    "\n",
    "# 讀取 CSV 檔案\n",
    "data = pd.read_csv(\"output.csv\")\n",
    "\n",
    "# 將 cluster=16 替換為 11\n",
    "data.loc[data['Value'] == 11, 'Value'] = 1\n",
    "\n",
    "\n",
    "\n",
    "# 儲存結果到新的 CSV 檔案\n",
    "data.to_csv(\"output_modified.csv\", index=False)\n",
    "\n",
    "print(\"處理完成，結果已儲存到 'output.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sophiehuang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to output_NB_1.csv\n"
     ]
    }
   ],
   "source": [
    "## Sophie Naive Bayes\n",
    "\n",
    "import os\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # type: ignore\n",
    "from nltk.corpus import stopwords  # type: ignore\n",
    "import nltk  # type: ignore\n",
    "import numpy as np  # type: ignore\n",
    "import chardet\n",
    "\n",
    "# 確保下載 NLTK 停用詞\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# 資料預處理\n",
    "def preprocess_text(text):\n",
    "    # 移除第三行\n",
    "    lines = text.split('\\n')\n",
    "    if len(lines) > 2:\n",
    "        text = '\\n'.join(lines[:2] + lines[3:])\n",
    "    # 轉換為小寫\n",
    "    text = text.lower()\n",
    "    # 移除標點符號\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # 移除指定單詞和停用詞\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words and word != 'embed']\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "# 載入文件\n",
    "def load_documents(folder_path):\n",
    "    \"\"\"從資料夾載入文件，並自動檢測編碼。\"\"\"\n",
    "    docs = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):  # 確保只處理文字檔\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            # 自動檢測文件編碼\n",
    "            with open(file_path, 'rb') as file:  # 以二進制方式打開\n",
    "                raw_data = file.read()\n",
    "                detected_encoding = chardet.detect(raw_data)['encoding']\n",
    "            \n",
    "            # 以檢測到的編碼打開文件\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding=detected_encoding) as file:\n",
    "                    doc_id = int(filename.split('.')[0])  # 假設檔名為數字\n",
    "                    docs[doc_id] = preprocess(file.read())\n",
    "            except UnicodeDecodeError:\n",
    "                print(f\"Failed to decode file {filename} with encoding {detected_encoding}. Skipping.\")\n",
    "    return docs\n",
    "\n",
    "\n",
    "# 計算 TF-IDF 並減少字彙數量\n",
    "def calculate_tfidf(docs, max_features=1000):\n",
    "    \"\"\"計算 TF-IDF 並選取前 max_features 個特徵。\"\"\"\n",
    "    vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "    tfidf_matrix = vectorizer.fit_transform([docs[doc_id] for doc_id in sorted(docs.keys())])\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "    return vocab\n",
    "\n",
    "\n",
    "# 計算似然比特徵選取\n",
    "def calculate_likelihood_ratio(docs, labels, vocab):\n",
    "    \"\"\"計算每個詞語和類別的似然比。\"\"\"\n",
    "    term_class_counts = Counter()\n",
    "    term_counts = Counter()\n",
    "    class_counts = Counter(labels)\n",
    "    total_docs = len(labels)\n",
    "\n",
    "    for i, doc in enumerate(docs):\n",
    "        unique_terms = set(doc.split())  # 文件中的唯一詞彙\n",
    "        for term in unique_terms:\n",
    "            if term in vocab:\n",
    "                term_class_counts[(term, labels[i])] += 1\n",
    "                term_counts[term] += 1\n",
    "\n",
    "    likelihood_scores = defaultdict(float)\n",
    "    for term in vocab:\n",
    "        for cls in class_counts.keys():\n",
    "            observed = term_class_counts.get((term, cls), 0)\n",
    "            expected = (term_counts[term] * class_counts[cls]) / total_docs\n",
    "            if expected > 0:  # 避免除以零\n",
    "                likelihood_scores[term] += ((observed - expected) ** 2) / expected\n",
    "\n",
    "    return sorted(likelihood_scores, key=likelihood_scores.get, reverse=True)\n",
    "\n",
    "\n",
    "# 訓練 Multinomial Naive Bayes\n",
    "def train_naive_bayes(class_docs, docs, selected_vocab):\n",
    "    \"\"\"使用加一平滑訓練 Multinomial Naive Bayes。\"\"\"\n",
    "    class_counts = Counter()\n",
    "    term_counts = {cls: Counter() for cls in class_docs.keys()}\n",
    "    vocab_size = len(selected_vocab)\n",
    "\n",
    "    for cls, doc_ids in class_docs.items():\n",
    "        class_counts[cls] += len(doc_ids)\n",
    "        for doc_id in doc_ids:\n",
    "            for term in docs[doc_id].split():\n",
    "                if term in selected_vocab:\n",
    "                    term_counts[cls][term] += 1\n",
    "\n",
    "    class_probs = {cls: math.log(class_counts[cls] / sum(class_counts.values())) for cls in class_counts}\n",
    "    term_probs = {cls: defaultdict(float) for cls in class_counts}\n",
    "\n",
    "    for cls in class_counts:\n",
    "        total_terms = sum(term_counts[cls].values()) + vocab_size\n",
    "        for term in selected_vocab:\n",
    "            term_probs[cls][term] = math.log((term_counts[cls][term] + 1) / total_terms)\n",
    "\n",
    "    return class_probs, term_probs\n",
    "\n",
    "\n",
    "# 文件分類\n",
    "def classify_document(doc, selected_vocab, class_probs, term_probs):\n",
    "    \"\"\"分類單一文件。\"\"\"\n",
    "    scores = {cls: class_probs[cls] for cls in class_probs}\n",
    "    for cls in class_probs:\n",
    "        for term in doc.split():\n",
    "            if term in selected_vocab:\n",
    "                scores[cls] += term_probs[cls][term]\n",
    "    return max(scores, key=scores.get)\n",
    "\n",
    "\n",
    "# 分類測試文件\n",
    "def classify_and_output_with_tfidf_likelihood(training_data, docs, output_path, tfidf_features=1000, lr_features=500):\n",
    "    \"\"\"使用 TF-IDF 和似然比進行文件分類並輸出。\"\"\"\n",
    "    class_docs = defaultdict(list)\n",
    "    labels = []\n",
    "    train_doc_ids = []\n",
    "    for line in open(training_data, 'r', encoding='utf-8').readlines():\n",
    "        parts = line.strip().split()\n",
    "        cls = int(parts[0])\n",
    "        doc_ids = list(map(int, parts[1:]))\n",
    "        class_docs[cls].extend(doc_ids)\n",
    "        labels.extend([cls] * len(doc_ids))\n",
    "        train_doc_ids.extend(doc_ids)\n",
    "\n",
    "    train_docs = [docs[doc_id] for doc_id in train_doc_ids]\n",
    "    all_doc_ids = set(docs.keys())\n",
    "    test_doc_ids = sorted(all_doc_ids - set(train_doc_ids))\n",
    "    test_docs = {doc_id: docs[doc_id] for doc_id in test_doc_ids}\n",
    "\n",
    "    tfidf_vocab = calculate_tfidf({doc_id: docs[doc_id] for doc_id in train_doc_ids}, max_features=tfidf_features)\n",
    "    lr_vocab = calculate_likelihood_ratio(train_docs, labels, tfidf_vocab)[:lr_features]\n",
    "\n",
    "    class_probs, term_probs = train_naive_bayes(class_docs, docs, lr_vocab)\n",
    "\n",
    "    test_predictions = {}\n",
    "    for doc_id, doc in test_docs.items():\n",
    "        predicted_class = classify_document(doc, lr_vocab, class_probs, term_probs)\n",
    "        test_predictions[doc_id] = predicted_class\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(\"Id,Value\\n\")\n",
    "        for doc_id in sorted(test_predictions.keys()):\n",
    "            file.write(f\"{doc_id},{test_predictions[doc_id]}\\n\")\n",
    "\n",
    "    print(f\"Results saved to {output_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    training_data = \"/Users/sophiehuang/Documents/113-1/113-1-IRTM/113-1-IRTM-final-project/folds/fold1.txt\"\n",
    "    folder_path = \"/Users/sophiehuang/Documents/113-1/113-1-IRTM/all_lyrics\"\n",
    "    output_path = \"output_NB_1.csv\"\n",
    "\n",
    "    docs = load_documents(folder_path)\n",
    "    classify_and_output_with_tfidf_likelihood(\n",
    "        training_data,\n",
    "        docs,\n",
    "        output_path,\n",
    "        tfidf_features=1000,\n",
    "        lr_features=500\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sophiehuang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to output_knn1.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "import numpy as np\n",
    "import chardet\n",
    "\n",
    "# 確保下載 NLTK 停用詞\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# 資料預處理\n",
    "def preprocess(text):\n",
    "    # 移除第三行\n",
    "    lines = text.split('\\n')\n",
    "    if len(lines) > 2:\n",
    "        text = '\\n'.join(lines[:2] + lines[3:])\n",
    "    # 轉換為小寫\n",
    "    text = text.lower()\n",
    "    # 移除標點符號\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # 移除指定單詞和停用詞\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words and word != 'embed']\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# 載入文件\n",
    "def load_documents(folder_path):\n",
    "    \"\"\"從資料夾載入文件，並自動檢測編碼。\"\"\"\n",
    "    docs = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):  # 確保只處理文字檔\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            # 自動檢測文件編碼\n",
    "            with open(file_path, 'rb') as file:  # 以二進制方式打開\n",
    "                raw_data = file.read()\n",
    "                detected_encoding = chardet.detect(raw_data)['encoding']\n",
    "            \n",
    "            # 以檢測到的編碼打開文件\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding=detected_encoding) as file:\n",
    "                    doc_id = int(filename.split('.')[0])  # 假設檔名為數字\n",
    "                    docs[doc_id] = preprocess(file.read())\n",
    "            except UnicodeDecodeError:\n",
    "                print(f\"Failed to decode file {filename} with encoding {detected_encoding}. Skipping.\")\n",
    "    return docs\n",
    "\n",
    "# 訓練和測試 kNN\n",
    "def classify_with_knn(training_data, docs, output_path, tfidf_features=1000, k=5):\n",
    "    \"\"\"使用 kNN 進行文件分類並輸出結果。\"\"\"\n",
    "    class_docs = defaultdict(list)\n",
    "    labels = []\n",
    "    train_doc_ids = []\n",
    "    \n",
    "    # 讀取訓練資料\n",
    "    for line in open(training_data, 'r', encoding='utf-8').readlines():\n",
    "        parts = line.strip().split()\n",
    "        cls = int(parts[0])\n",
    "        doc_ids = list(map(int, parts[1:]))\n",
    "        class_docs[cls].extend(doc_ids)\n",
    "        labels.extend([cls] * len(doc_ids))\n",
    "        train_doc_ids.extend(doc_ids)\n",
    "    \n",
    "    # 將訓練和測試分開\n",
    "    all_doc_ids = set(docs.keys())\n",
    "    test_doc_ids = sorted(all_doc_ids - set(train_doc_ids))\n",
    "    test_docs = {doc_id: docs[doc_id] for doc_id in test_doc_ids}\n",
    "    \n",
    "    # 計算 TF-IDF\n",
    "    vectorizer = TfidfVectorizer(max_features=tfidf_features)\n",
    "    X_train = vectorizer.fit_transform([docs[doc_id] for doc_id in train_doc_ids])\n",
    "    X_test = vectorizer.transform([test_docs[doc_id] for doc_id in test_doc_ids])\n",
    "    y_train = np.array(labels)\n",
    "\n",
    "    # 建立 kNN 模型\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # 預測測試資料\n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    # 儲存結果\n",
    "    with open(output_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(\"Id,Value\\n\")\n",
    "        for doc_id, pred in zip(test_doc_ids, y_pred):\n",
    "            file.write(f\"{doc_id},{pred}\\n\")\n",
    "    \n",
    "    print(f\"Results saved to {output_path}\")\n",
    "\n",
    "# 主程式\n",
    "if __name__ == \"__main__\":\n",
    "    training_data = \"/Users/sophiehuang/Documents/113-1/113-1-IRTM/113-1-IRTM-final-project/folds/fold1.txt\"\n",
    "    folder_path = \"/Users/sophiehuang/Documents/113-1/113-1-IRTM/all_lyrics\"\n",
    "    output_path = \"output_knn1.csv\"\n",
    "\n",
    "    docs = load_documents(folder_path)\n",
    "    classify_with_knn(\n",
    "        training_data,\n",
    "        docs,\n",
    "        output_path,\n",
    "        tfidf_features=1000,\n",
    "        k=5\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sophiehuang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to output_rocchio1.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import chardet\n",
    "import nltk\n",
    "\n",
    "# 確保下載 NLTK 停用詞\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# 資料預處理\n",
    "def preprocess(text):\n",
    "    # 移除第三行\n",
    "    lines = text.split('\\n')\n",
    "    if len(lines) > 2:\n",
    "        text = '\\n'.join(lines[:2] + lines[3:])\n",
    "    # 轉換為小寫\n",
    "    text = text.lower()\n",
    "    # 移除標點符號\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # 移除指定單詞和停用詞\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words and word != 'embed']\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# 載入文件\n",
    "def load_documents(folder_path):\n",
    "    \"\"\"從資料夾載入文件，並自動檢測編碼。\"\"\"\n",
    "    docs = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):  # 確保只處理文字檔\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            # 自動檢測文件編碼\n",
    "            with open(file_path, 'rb') as file:  # 以二進制方式打開\n",
    "                raw_data = file.read()\n",
    "                detected_encoding = chardet.detect(raw_data)['encoding']\n",
    "            \n",
    "            # 以檢測到的編碼打開文件\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding=detected_encoding) as file:\n",
    "                    doc_id = int(filename.split('.')[0])  # 假設檔名為數字\n",
    "                    docs[doc_id] = preprocess(file.read())\n",
    "            except UnicodeDecodeError:\n",
    "                print(f\"Failed to decode file {filename} with encoding {detected_encoding}. Skipping.\")\n",
    "    return docs\n",
    "\n",
    "# 訓練 Rocchio 分類器\n",
    "def train_rocchio(training_data, docs, tfidf_features=1000):\n",
    "    \"\"\"使用 Rocchio 方法計算每個類別的質心。\"\"\"\n",
    "    class_docs = defaultdict(list)\n",
    "    labels = []\n",
    "    train_doc_ids = []\n",
    "    \n",
    "    # 讀取訓練資料\n",
    "    for line in open(training_data, 'r', encoding='utf-8').readlines():\n",
    "        parts = line.strip().split()\n",
    "        cls = int(parts[0])\n",
    "        doc_ids = list(map(int, parts[1:]))\n",
    "        class_docs[cls].extend(doc_ids)\n",
    "        labels.extend([cls] * len(doc_ids))\n",
    "        train_doc_ids.extend(doc_ids)\n",
    "    \n",
    "    # 計算 TF-IDF\n",
    "    vectorizer = TfidfVectorizer(max_features=tfidf_features)\n",
    "    X_train = vectorizer.fit_transform([docs[doc_id] for doc_id in train_doc_ids])\n",
    "    y_train = np.array(labels)\n",
    "    \n",
    "    # 計算每個類別的質心\n",
    "    centroids = {}\n",
    "    for cls in class_docs:\n",
    "        cls_indices = [i for i, label in enumerate(y_train) if label == cls]\n",
    "        cls_vectors = X_train[cls_indices]\n",
    "        centroids[cls] = cls_vectors.mean(axis=0)  # 計算質心\n",
    "    \n",
    "    return centroids, vectorizer\n",
    "\n",
    "# 文件分類\n",
    "def classify_with_rocchio(test_docs, centroids, vectorizer):\n",
    "    \"\"\"使用 Rocchio 分類器對測試文件進行分類。\"\"\"\n",
    "    X_test = vectorizer.transform([test_docs[doc_id] for doc_id in sorted(test_docs.keys())])\n",
    "    test_predictions = {}\n",
    "\n",
    "    for idx, doc_id in enumerate(sorted(test_docs.keys())):\n",
    "        distances = {}\n",
    "        for cls, centroid in centroids.items():\n",
    "            # 計算與每個類別質心的距離（歐幾里得距離）\n",
    "            distances[cls] = np.linalg.norm(X_test[idx] - centroid)\n",
    "        predicted_class = min(distances, key=distances.get)  # 選擇距離最近的類別\n",
    "        test_predictions[doc_id] = predicted_class\n",
    "    \n",
    "    return test_predictions\n",
    "\n",
    "# 主程式\n",
    "if __name__ == \"__main__\":\n",
    "    training_data = \"/Users/sophiehuang/Documents/113-1/113-1-IRTM/113-1-IRTM-final-project/folds/fold1.txt\"\n",
    "    folder_path = \"/Users/sophiehuang/Documents/113-1/113-1-IRTM/all_lyrics\"\n",
    "    output_path = \"output_rocchio1.csv\"\n",
    "\n",
    "    # 載入資料\n",
    "    docs = load_documents(folder_path)\n",
    "\n",
    "    # 訓練 Rocchio 分類器\n",
    "    centroids, vectorizer = train_rocchio(\n",
    "        training_data,\n",
    "        docs,\n",
    "        tfidf_features=1000\n",
    "    )\n",
    "\n",
    "    # 獲取測試文件\n",
    "    all_doc_ids = set(docs.keys())\n",
    "    train_doc_ids = []\n",
    "    for line in open(training_data, 'r', encoding='utf-8').readlines():\n",
    "        parts = line.strip().split()\n",
    "        train_doc_ids.extend(list(map(int, parts[1:])))\n",
    "    test_doc_ids = sorted(all_doc_ids - set(train_doc_ids))\n",
    "    test_docs = {doc_id: docs[doc_id] for doc_id in test_doc_ids}\n",
    "\n",
    "    # 使用 Rocchio 分類測試文件\n",
    "    test_predictions = classify_with_rocchio(test_docs, centroids, vectorizer)\n",
    "\n",
    "    # 儲存結果\n",
    "    with open(output_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(\"Id,Value\\n\")\n",
    "        for doc_id in sorted(test_predictions.keys()):\n",
    "            file.write(f\"{doc_id},{test_predictions[doc_id]}\\n\")\n",
    "\n",
    "    print(f\"Results saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sophiehuang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to output_svm1.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import chardet\n",
    "import nltk\n",
    "\n",
    "# 確保下載 NLTK 停用詞\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# 資料預處理\n",
    "def preprocess(text):\n",
    "    # 移除第三行\n",
    "    lines = text.split('\\n')\n",
    "    if len(lines) > 2:\n",
    "        text = '\\n'.join(lines[:2] + lines[3:])\n",
    "    # 轉換為小寫\n",
    "    text = text.lower()\n",
    "    # 移除標點符號\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # 移除指定單詞和停用詞\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words and word != 'embed']\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# 載入文件\n",
    "def load_documents(folder_path):\n",
    "    \"\"\"從資料夾載入文件，並自動檢測編碼。\"\"\"\n",
    "    docs = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):  # 確保只處理文字檔\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            # 自動檢測文件編碼\n",
    "            with open(file_path, 'rb') as file:  # 以二進制方式打開\n",
    "                raw_data = file.read()\n",
    "                detected_encoding = chardet.detect(raw_data)['encoding']\n",
    "            \n",
    "            # 以檢測到的編碼打開文件\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding=detected_encoding) as file:\n",
    "                    doc_id = int(filename.split('.')[0])  # 假設檔名為數字\n",
    "                    docs[doc_id] = preprocess(file.read())\n",
    "            except UnicodeDecodeError:\n",
    "                print(f\"Failed to decode file {filename} with encoding {detected_encoding}. Skipping.\")\n",
    "    return docs\n",
    "\n",
    "# 訓練 SVM 分類器\n",
    "def train_svm(training_data, docs, tfidf_features=1000, kernel='linear'):\n",
    "    \"\"\"使用支持向量機訓練模型。\"\"\"\n",
    "    class_docs = defaultdict(list)\n",
    "    labels = []\n",
    "    train_doc_ids = []\n",
    "    \n",
    "    # 讀取訓練資料\n",
    "    for line in open(training_data, 'r', encoding='utf-8').readlines():\n",
    "        parts = line.strip().split()\n",
    "        cls = int(parts[0])\n",
    "        doc_ids = list(map(int, parts[1:]))\n",
    "        class_docs[cls].extend(doc_ids)\n",
    "        labels.extend([cls] * len(doc_ids))\n",
    "        train_doc_ids.extend(doc_ids)\n",
    "    \n",
    "    # 計算 TF-IDF\n",
    "    vectorizer = TfidfVectorizer(max_features=tfidf_features)\n",
    "    X_train = vectorizer.fit_transform([docs[doc_id] for doc_id in train_doc_ids])\n",
    "    y_train = np.array(labels)\n",
    "    \n",
    "    # 訓練 SVM 模型\n",
    "    svm_model = SVC(kernel=kernel, probability=True, random_state=42)\n",
    "    svm_model.fit(X_train, y_train)\n",
    "    \n",
    "    return svm_model, vectorizer\n",
    "\n",
    "# 文件分類\n",
    "def classify_with_svm(test_docs, svm_model, vectorizer):\n",
    "    \"\"\"使用 SVM 對測試文件進行分類。\"\"\"\n",
    "    X_test = vectorizer.transform([test_docs[doc_id] for doc_id in sorted(test_docs.keys())])\n",
    "    test_predictions = {}\n",
    "\n",
    "    for idx, doc_id in enumerate(sorted(test_docs.keys())):\n",
    "        predicted_class = svm_model.predict(X_test[idx])\n",
    "        test_predictions[doc_id] = predicted_class[0]  # SVM 的預測值是一個陣列\n",
    "    \n",
    "    return test_predictions\n",
    "\n",
    "# 主程式\n",
    "if __name__ == \"__main__\":\n",
    "    training_data = \"/Users/sophiehuang/Documents/113-1/113-1-IRTM/113-1-IRTM-final-project/folds/fold2.txt\"\n",
    "    folder_path = \"/Users/sophiehuang/Documents/113-1/113-1-IRTM/all_lyrics\"\n",
    "    output_path = \"output_svm1.csv\"\n",
    "\n",
    "    # 載入資料\n",
    "    docs = load_documents(folder_path)\n",
    "\n",
    "    # 訓練 SVM 分類器\n",
    "    svm_model, vectorizer = train_svm(\n",
    "        training_data,\n",
    "        docs,\n",
    "        tfidf_features=1000,\n",
    "        kernel='linear'\n",
    "    )\n",
    "\n",
    "    # 獲取測試文件\n",
    "    all_doc_ids = set(docs.keys())\n",
    "    train_doc_ids = []\n",
    "    for line in open(training_data, 'r', encoding='utf-8').readlines():\n",
    "        parts = line.strip().split()\n",
    "        train_doc_ids.extend(list(map(int, parts[1:])))\n",
    "    test_doc_ids = sorted(all_doc_ids - set(train_doc_ids))\n",
    "    test_docs = {doc_id: docs[doc_id] for doc_id in test_doc_ids}\n",
    "\n",
    "    # 使用 SVM 分類測試文件\n",
    "    test_predictions = classify_with_svm(test_docs, svm_model, vectorizer)\n",
    "\n",
    "    # 儲存結果\n",
    "    with open(output_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(\"Id,Value\\n\")\n",
    "        for doc_id in sorted(test_predictions.keys()):\n",
    "            file.write(f\"{doc_id},{test_predictions[doc_id]}\\n\")\n",
    "\n",
    "    print(f\"Results saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2187819926.py, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[74], line 26\u001b[0;36m\u001b[0m\n\u001b[0;31m    tokens = [word for word in tokens if word not in stop_words and word != 'embed', 'oh', 'yeah', 'baby', 'uh', 'la', 'na', 'ah', 'whoa', 'gonna', 'wanna', 'hey', 'ho', 'ha', 'll']\u001b[0m\n\u001b[0m                                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import chardet\n",
    "import nltk\n",
    "\n",
    "# 確保下載 NLTK 停用詞\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# 資料預處理\n",
    "def preprocess(text):\n",
    "    # 移除第三行\n",
    "    lines = text.split('\\n')\n",
    "    if len(lines) > 2:\n",
    "        text = '\\n'.join(lines[:2] + lines[3:])\n",
    "    # 轉換為小寫\n",
    "    text = text.lower()\n",
    "    # 移除標點符號\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # 移除指定單詞和停用詞\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    additional_stop_words = ['embed', 'oh', 'yeah', 'baby', 'uh', 'la', 'na', 'ah', 'whoa', 'gonna', 'wanna', 'hey', 'ho', 'ha', 'll']\n",
    "    tokens = [word for word in tokens if word not in stop_words and word not in additional_stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# 載入文件\n",
    "def load_documents(folder_path):\n",
    "    \"\"\"從資料夾載入文件，並自動檢測編碼。\"\"\"\n",
    "    docs = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):  # 確保只處理文字檔\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            # 自動檢測文件編碼\n",
    "            with open(file_path, 'rb') as file:  # 以二進制方式打開\n",
    "                raw_data = file.read()\n",
    "                detected_encoding = chardet.detect(raw_data)['encoding']\n",
    "            \n",
    "            # 以檢測到的編碼打開文件\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding=detected_encoding) as file:\n",
    "                    doc_id = int(filename.split('.')[0])  # 假設檔名為數字\n",
    "                    docs[doc_id] = preprocess(file.read())\n",
    "            except UnicodeDecodeError:\n",
    "                print(f\"Failed to decode file {filename} with encoding {detected_encoding}. Skipping.\")\n",
    "    return docs\n",
    "\n",
    "# 訓練 SVM 分類器\n",
    "def train_svm(training_data, docs, tfidf_features=1000, kernel='linear', C=1.0, gamma='scale', degree=3):\n",
    "    \"\"\"使用支持向量機訓練模型，並支持參數設置。\"\"\"\n",
    "    class_docs = defaultdict(list)\n",
    "    labels = []\n",
    "    train_doc_ids = []\n",
    "    \n",
    "    # 讀取訓練資料\n",
    "    for line in open(training_data, 'r', encoding='utf-8').readlines():\n",
    "        parts = line.strip().split()\n",
    "        cls = int(parts[0])\n",
    "        doc_ids = list(map(int, parts[1:]))\n",
    "        class_docs[cls].extend(doc_ids)\n",
    "        labels.extend([cls] * len(doc_ids))\n",
    "        train_doc_ids.extend(doc_ids)\n",
    "    \n",
    "    # 計算 TF-IDF\n",
    "    vectorizer = TfidfVectorizer(max_features=tfidf_features)\n",
    "    X_train = vectorizer.fit_transform([docs[doc_id] for doc_id in train_doc_ids])\n",
    "    y_train = np.array(labels)\n",
    "    \n",
    "    # 訓練 SVM 模型\n",
    "    svm_model = SVC(kernel=kernel, C=C, gamma=gamma, degree=degree, probability=True, random_state=42)\n",
    "    svm_model.fit(X_train, y_train)\n",
    "    \n",
    "    return svm_model, vectorizer\n",
    "\n",
    "# 文件分類\n",
    "def classify_with_svm(test_docs, svm_model, vectorizer):\n",
    "    \"\"\"使用 SVM 對測試文件進行分類。\"\"\"\n",
    "    X_test = vectorizer.transform([test_docs[doc_id] for doc_id in sorted(test_docs.keys())])\n",
    "    test_predictions = {}\n",
    "\n",
    "    for idx, doc_id in enumerate(sorted(test_docs.keys())):\n",
    "        predicted_class = svm_model.predict(X_test[idx])\n",
    "        test_predictions[doc_id] = predicted_class[0]  # SVM 的預測值是一個陣列\n",
    "    \n",
    "    return test_predictions\n",
    "\n",
    "# 主程式\n",
    "if __name__ == \"__main__\":\n",
    "    training_data = \"/Users/sophiehuang/Documents/113-1/113-1-IRTM/113-1-IRTM-final-project/folds/fold1.txt\"\n",
    "    folder_path = \"/Users/sophiehuang/Documents/113-1/113-1-IRTM/all_lyrics\"\n",
    "    output_path = \"output_svm_param_stop_1.csv\"\n",
    "\n",
    "    # 載入資料\n",
    "    docs = load_documents(folder_path)\n",
    "\n",
    "    # 訓練 SVM 分類器\n",
    "    svm_model, vectorizer = train_svm(\n",
    "        training_data,\n",
    "        docs,\n",
    "        tfidf_features=1000,\n",
    "        kernel='rbf',  # 核函數選擇：'linear', 'rbf', 'poly', 'sigmoid'\n",
    "        C=1.0,  # 正則化參數\n",
    "        gamma='scale',  # 控制非線性核的範圍 ('scale' 自適應選擇)\n",
    "        degree=3  # 多項式核的次數\n",
    "    )\n",
    "\n",
    "    # 獲取測試文件\n",
    "    all_doc_ids = set(docs.keys())\n",
    "    train_doc_ids = []\n",
    "    for line in open(training_data, 'r', encoding='utf-8').readlines():\n",
    "        parts = line.strip().split()\n",
    "        train_doc_ids.extend(list(map(int, parts[1:])))\n",
    "    test_doc_ids = sorted(all_doc_ids - set(train_doc_ids))\n",
    "    test_docs = {doc_id: docs[doc_id] for doc_id in test_doc_ids}\n",
    "\n",
    "    # 使用 SVM 分類測試文件\n",
    "    test_predictions = classify_with_svm(test_docs, svm_model, vectorizer)\n",
    "\n",
    "    # 儲存結果\n",
    "    with open(output_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(\"Id,Value\\n\")\n",
    "        for doc_id in sorted(test_predictions.keys()):\n",
    "            file.write(f\"{doc_id},{test_predictions[doc_id]}\\n\")\n",
    "\n",
    "    print(f\"Results saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b848153359c44a1bc9af60793583b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/sophiehuang/anaconda3/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 6 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 147\u001b[0m\n\u001b[1;32m    144\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# 訓練模型\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# 儲存模型\u001b[39;00m\n\u001b[1;32m    150\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./bert_emotion_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[58], line 79\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, val_loader, model, optimizer, epochs)\u001b[0m\n\u001b[1;32m     76\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     77\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 79\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     81\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1587\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1585\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mproblem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle_label_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1586\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m-> 1587\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mproblem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_label_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1589\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m BCEWithLogitsLoss()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3013\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3014\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 6 is out of bounds."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 確保 GPU 可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 自定義數據集\n",
    "class LyricsDataset(Dataset):\n",
    "    def __init__(self, lyrics, labels, tokenizer, max_length):\n",
    "        self.lyrics = lyrics\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lyrics)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        lyric = self.lyrics[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            lyric,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "# 加載數據\n",
    "def load_data(folder_path, training_data):\n",
    "    # 讀取歌詞\n",
    "    lyrics = []\n",
    "    labels = []\n",
    "\n",
    "    for line in open(training_data, \"r\", encoding=\"utf-8\").readlines():\n",
    "        parts = line.strip().split()\n",
    "        cls = int(parts[0])\n",
    "        doc_ids = list(map(int, parts[1:]))\n",
    "        labels.extend([cls] * len(doc_ids))\n",
    "\n",
    "        for doc_id in doc_ids:\n",
    "            file_path = os.path.join(folder_path, f\"{doc_id}.txt\")\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                lyrics.append(file.read().strip())\n",
    "\n",
    "    return lyrics, labels\n",
    "\n",
    "# 模型訓練\n",
    "def train_model(train_loader, val_loader, model, optimizer, epochs=3):\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Training Loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        # 評估模型\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        print(f\"Validation Loss: {val_loss / len(val_loader):.4f}\")\n",
    "        print(classification_report(val_labels, val_preds))\n",
    "\n",
    "    return model\n",
    "\n",
    "# 主程式\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"/Users/sophiehuang/Documents/113-1/113-1-IRTM/all_lyrics\"  # 替換為歌詞資料夾的路徑\n",
    "    training_data = \"/Users/sophiehuang/Documents/113-1/113-1-IRTM/113-1-IRTM-final-project/folds/fold1.txt\"  # 替換為訓練數據文件的路徑\n",
    "    max_length = 128  # BERT 最大文本長度\n",
    "    batch_size = 16\n",
    "    learning_rate = 2e-5\n",
    "    epochs = 3\n",
    "    num_labels = 6  # 替換為情緒類別數量\n",
    "\n",
    "    # 加載數據\n",
    "    lyrics, labels = load_data(folder_path, training_data)\n",
    "\n",
    "    # 分割數據集\n",
    "    train_lyrics, val_lyrics, train_labels, val_labels = train_test_split(\n",
    "        lyrics, labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # 初始化 Tokenizer 和模型\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\", num_labels=num_labels\n",
    "    )\n",
    "\n",
    "    # 構建 DataLoader\n",
    "    train_dataset = LyricsDataset(train_lyrics, train_labels, tokenizer, max_length)\n",
    "    val_dataset = LyricsDataset(val_lyrics, val_labels, tokenizer, max_length)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # 優化器\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 訓練模型\n",
    "    model = train_model(train_loader, val_loader, model, optimizer, epochs)\n",
    "\n",
    "    # 儲存模型\n",
    "    model.save_pretrained(\"./bert_emotion_model\")\n",
    "    tokenizer.save_pretrained(\"./bert_emotion_model\")\n",
    "    print(\"Model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# 讀取 txt 檔案\n",
    "with open(\".txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# 初始化資料列表\n",
    "rows = []\n",
    "\n",
    "# 解析每一行的內容\n",
    "for line in lines:\n",
    "    line = line.strip()  # 移除行首和行尾空白字元\n",
    "    if \":\" in line:  # 確保該行包含 ':'\n",
    "        try:\n",
    "            # 分割出 cluster ID 和 doc_ids\n",
    "            cluster, doc_ids = line.split(\":\")\n",
    "            cluster_id = cluster.strip().split(\" \")[1]  # 取出 Cluster 編號\n",
    "            doc_ids = doc_ids.split(\",\")  # 分割出 doc_id 列表\n",
    "\n",
    "            # 將 cluster ID 和每個 doc_id 儲存為一行資料\n",
    "            for doc_id in doc_ids:\n",
    "                if doc_id.strip():  # 確保 doc_id 不為空\n",
    "                    rows.append({\"doc_id\": doc_id.strip(), \"cluster\": cluster_id})\n",
    "        except Exception as e:\n",
    "            print(f\"解析行時出現問題：{line}，錯誤訊息：{e}\")\n",
    "\n",
    "# 確保資料非空\n",
    "if rows:\n",
    "    # 寫入 CSV 檔案\n",
    "    with open(\"output.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=[\"doc_id\", \"cluster\"])\n",
    "        writer.writeheader()  # 寫入欄位名稱\n",
    "        writer.writerows(rows)  # 寫入資料\n",
    "\n",
    "    print(\"CSV 檔案已成功建立：output.csv\")\n",
    "else:\n",
    "    print(\"未找到有效資料，請檢查 .txt 檔案的內容格式！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 檔案已成功建立且已根據 doc_id 排序：output.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# 讀取 txt 檔案\n",
    "with open(\"filtered_clusters.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# 初始化資料列表\n",
    "rows = []\n",
    "\n",
    "# 解析每一行的內容\n",
    "for line in lines:\n",
    "    line = line.strip()  # 移除行首和行尾空白字元\n",
    "    if \":\" in line:  # 確保該行包含 ':'\n",
    "        try:\n",
    "            # 分割出 cluster ID 和 doc_ids\n",
    "            cluster, doc_ids = line.split(\":\")\n",
    "            cluster_id = cluster.strip().split(\" \")[1]  # 取出 Cluster 編號\n",
    "            doc_ids = doc_ids.split(\",\")  # 分割出 doc_id 列表\n",
    "\n",
    "            # 將 cluster ID 和每個 doc_id 儲存為一行資料\n",
    "            for doc_id in doc_ids:\n",
    "                if doc_id.strip():  # 確保 doc_id 不為空\n",
    "                    rows.append({\"doc_id\": int(doc_id.strip()), \"cluster\": cluster_id})  # 將 doc_id 轉為數字排序\n",
    "        except Exception as e:\n",
    "            print(f\"解析行時出現問題：{line}，錯誤訊息：{e}\")\n",
    "\n",
    "# 對資料根據 doc_id 進行排序\n",
    "rows = sorted(rows, key=lambda x: x[\"doc_id\"])\n",
    "\n",
    "# 確保資料非空\n",
    "if rows:\n",
    "    # 寫入 CSV 檔案\n",
    "    with open(\"output.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=[\"doc_id\", \"cluster\"])\n",
    "        writer.writeheader()  # 寫入欄位名稱\n",
    "        writer.writerows(rows)  # 寫入資料\n",
    "\n",
    "    print(\"CSV 檔案已成功建立且已根據 doc_id 排序：output.csv\")\n",
    "else:\n",
    "    print(\"未找到有效資料，請檢查 .txt 檔案的內容格式！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "資料預覽：\n",
      "   doc_id  cluster\n",
      "0       1        4\n",
      "1       2        5\n",
      "2       3        1\n",
      "3       4        1\n",
      "4       5        4\n",
      "chunk_1.csv 已儲存至 label_chunks\n",
      "chunk_2.csv 已儲存至 label_chunks\n",
      "chunk_3.csv 已儲存至 label_chunks\n",
      "chunk_4.csv 已儲存至 label_chunks\n",
      "chunk_5.csv 已儲存至 label_chunks\n",
      "chunk_6.csv 已儲存至 label_chunks\n",
      "chunk_7.csv 已儲存至 label_chunks\n",
      "chunk_8.csv 已儲存至 label_chunks\n",
      "chunk_9.csv 已儲存至 label_chunks\n",
      "chunk_10.csv 已儲存至 label_chunks\n",
      "資料已按 label 均分為 10 等份，結果存放於資料夾 'label_chunks'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 讀取 CSV 檔案\n",
    "data = pd.read_csv(\"output.csv\")\n",
    "\n",
    "# 檢查輸入資料\n",
    "print(\"資料預覽：\")\n",
    "print(data.head())\n",
    "\n",
    "# 創建存放分割資料的資料夾\n",
    "output_folder = \"label_chunks\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 儲存分割結果\n",
    "chunk_files = {f\"chunk_{i+1}.csv\": [] for i in range(10)}\n",
    "\n",
    "# 對每個 cluster 進行分割\n",
    "for label, group in data.groupby(\"cluster\"):\n",
    "    # 將該 label 的資料均分為 10 等份\n",
    "    chunk_size = len(group) // 10\n",
    "    remainder = len(group) % 10  # 用於處理無法整除的部分\n",
    "    start_idx = 0\n",
    "    \n",
    "    for i in range(10):\n",
    "        if i < remainder:  # 前 remainder 等份多分配一筆資料\n",
    "            end_idx = start_idx + chunk_size + 1\n",
    "        else:\n",
    "            end_idx = start_idx + chunk_size\n",
    "        \n",
    "        chunk = group.iloc[start_idx:end_idx]\n",
    "        chunk_files[f\"chunk_{i+1}.csv\"].append(chunk)\n",
    "        start_idx = end_idx\n",
    "\n",
    "# 合併並寫入每一個 chunk 到對應的檔案\n",
    "for chunk_name, chunk_data in chunk_files.items():\n",
    "    combined_chunk = pd.concat(chunk_data)\n",
    "    combined_chunk.to_csv(os.path.join(output_folder, chunk_name), index=False)\n",
    "    print(f\"{chunk_name} 已儲存至 {output_folder}\")\n",
    "\n",
    "print(\"資料已按 label 均分為 10 等份，結果存放於資料夾 'label_chunks'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping line (no colon found): \n",
      "Skipping line (no colon found): \n",
      "Skipping line (no colon found): \n",
      "Skipping line (no colon found): \n",
      "Skipping line (no colon found): \n",
      "Skipping line (no colon found): \n",
      "Skipping line (no colon found): \n",
      "Skipping line (no colon found): \n",
      "已成功將結果保存到 filtered_clusters.txt\n"
     ]
    }
   ],
   "source": [
    "def load_file(filepath):\n",
    "    \"\"\"從文件中載入數據\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return file.readlines()\n",
    "\n",
    "def save_file(filepath, data):\n",
    "    \"\"\"將數據保存到文件\"\"\"\n",
    "    with open(filepath, 'w', encoding='utf-8') as file:\n",
    "        file.writelines(data)\n",
    "\n",
    "def filter_ids(original_file, ids_to_remove_file, output_file):\n",
    "    \"\"\"過濾掉指定的 ID\"\"\"\n",
    "    # 讀取原始數據\n",
    "    original_data = load_file(original_file)\n",
    "\n",
    "    # 讀取要刪除的 ID\n",
    "    with open(ids_to_remove_file, 'r', encoding='utf-8') as file:\n",
    "        ids_to_remove = {line.strip().split('.')[0] for line in file.readlines()}\n",
    "\n",
    "    # 過濾數據\n",
    "    filtered_data = []\n",
    "    for line in original_data:\n",
    "        # 確保行中包含冒號\n",
    "        if ':' not in line:\n",
    "            print(f\"Skipping line (no colon found): {line.strip()}\")\n",
    "            continue\n",
    "\n",
    "        cluster_id, ids = line.split(':', 1)  # 僅分割第一個冒號\n",
    "        ids_list = ids.split(',')\n",
    "        # 過濾掉需要刪除的 ID\n",
    "        filtered_ids = [id.strip() for id in ids_list if id.strip() not in ids_to_remove]\n",
    "\n",
    "        # 如果該 cluster 沒有剩餘 ID，可選擇是否跳過\n",
    "        if not filtered_ids:\n",
    "            print(f\"Skipping cluster {cluster_id} (no IDs left after filtering).\")\n",
    "            continue\n",
    "\n",
    "        # 構建新的行\n",
    "        filtered_data.append(f\"{cluster_id}: {', '.join(filtered_ids)}\\n\")\n",
    "\n",
    "    # 保存過濾後的數據\n",
    "    save_file(output_file, filtered_data)\n",
    "    print(f\"已成功將結果保存到 {output_file}\")\n",
    "\n",
    "\n",
    "# 設定檔案路徑\n",
    "original_file = 'cluster.txt'       # 原始數據檔案\n",
    "ids_to_remove_file = 'remove_ids.txt'  # 包含需要刪除 ID 的檔案\n",
    "output_file = 'filtered_clusters.txt'  # 過濾後的數據檔案\n",
    "\n",
    "# 執行過濾\n",
    "filter_ids(original_file, ids_to_remove_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ids(original_file, ids_to_remove_file, output_file):\n",
    "    \"\"\"過濾掉指定的 ID\"\"\"\n",
    "    # 讀取原始數據\n",
    "    original_data = load_file(original_file)\n",
    "\n",
    "    # 讀取要刪除的 ID\n",
    "    with open(ids_to_remove_file, 'r', encoding='utf-8') as file:\n",
    "        ids_to_remove = {line.strip().split('.')[0] for line in file.readlines()}\n",
    "\n",
    "    # 過濾數據\n",
    "    filtered_data = []\n",
    "    for line in original_data:\n",
    "        # 確保行中包含冒號\n",
    "        if ':' not in line:\n",
    "            print(f\"Skipping line (no colon found): {line.strip()}\")\n",
    "            continue\n",
    "\n",
    "        cluster_id, ids = line.split(':', 1)  # 僅分割第一個冒號\n",
    "        ids_list = ids.split(',')\n",
    "        # 過濾掉需要刪除的 ID\n",
    "        filtered_ids = [id.strip() for id in ids_list if id.strip() not in ids_to_remove]\n",
    "\n",
    "        # 如果該 cluster 沒有剩餘 ID，可選擇是否跳過\n",
    "        if not filtered_ids:\n",
    "            print(f\"Skipping cluster {cluster_id} (no IDs left after filtering).\")\n",
    "            continue\n",
    "\n",
    "        # 構建新的行\n",
    "        filtered_data.append(f\"{cluster_id}: {', '.join(filtered_ids)}\\n\")\n",
    "\n",
    "    # 保存過濾後的數據\n",
    "    save_file(output_file, filtered_data)\n",
    "    print(f\"已成功將結果保存到 {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
